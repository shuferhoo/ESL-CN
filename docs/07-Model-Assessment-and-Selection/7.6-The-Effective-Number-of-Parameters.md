# 7.6 参数的有效个数

| 原文   | [The Elements of Statistical Learning](../book/The Elements of Statistical Learning.pdf) |
| ---- | ---------------------------------------- |
| 翻译   | szcf-weiya                               |
| 时间   | 2017-02-18:2017-02-18                    |
|更新|2017-09-17& 2018-01-05|

“参数个数”概念可以推广，特别是推广到在拟合中使用了正则的模型中。假设我们将输出$y_1,y_2,\ldots,y_N$压栈到向量$\mathbf y$中，类似地对预测值进行同样操作得到$\hat{\mathbf y}$。于是我们可以将线性拟合模型写成

$$
\mathbf{\hat y=Sy}\tag{7.31}
$$

其中$\mathbf S$为依赖于输入向量$x_i$但不依赖于输出$y_i$的 $N\times N$阶矩阵。线性拟合方法包括在原始特征或在导出基的集合中运用的线性回归，以及采用平方收缩的光滑化方法，比如岭回归和三次光滑样条。则有效参数个数（effective number of parameters）定义为

$$
df(\mathbf S)=trace(\mathbf S)\tag{7.32}
$$

是$\mathbf S$对角元之和（也被称作有效自由度effective degrees-of-freedom）。注意到如果$\mathbf S$为投影到由$M$个特征张开的基础集（basis set）上的正交投影矩阵，则$trace(\mathbf S)=M$。事实证明$trace(\mathbf S)$恰巧是$C_p$统计量（7.26）中替换掉$d$作为参数个数的那个值。

!!! note "weiya注： Recall (7.26)"
    $$
    C_p=\overline{err}+2\cdot\frac{d}{N}\hat \sigma^2_\varepsilon\tag{7.26}
    $$

如果$\mathbf y$是从加性误差模型$Y=f(X)+\varepsilon$中产生的，$Var(\varepsilon)=\sigma_\epsilon^2$，则可以证明$\sum_{i=1}^NCov(\hat y_i,y_i)=trace(\mathbf S)\sigma_{\varepsilon}^2$，导出了更一般的定义

$$
df(\hat {\mathbf y})=\frac{\sum_{i=1}^NCov(\hat y_i,y_i)}{\sigma_\varepsilon^2}\tag{7.33}
$$

（练习7.4和7.5）。153页的5.4.1节给出了在光滑样条情形下$df=trace(\mathbf S)$更直观的定义。

对于像神经网络的模型，我们用系数衰减（正则化）$\alpha\sum_m w_m^2$来最小化误差函数$R(w)$，有效参数个数有如下形式

$$
df(\alpha)=\sum\limits_{i=1}^M\frac{\theta_m}{\theta_m+\alpha}\tag{7.34}
$$

其中$\theta_m$是Hessian矩阵$\partial^2R(w)/\partial w\partial w^T$的特征值。如果我们对解的误差函数做二次近似便可由式（7.32）导出式（7.34）（Bishop，1995[^1]）

[^1]: Bishop, C. (1995). Neural Networks for Pattern Recognition, Clarendon Press, Oxford.
